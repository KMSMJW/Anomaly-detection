{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import processing as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len = 5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = tf.Variable(tf.zeros([max_len,d_model], tf.float32), trainable=False) # pe = (max_len,d_model)\n",
    "        position = tf.range(0,max_len,dtype=tf.float32)[:, tf.newaxis] # position = (max_len,1)\n",
    "        div_term = tf.math.exp(tf.range(0,d_model,2,tf.float32)*-(tf.math.log(10000.0)/d_model)) # div_term = (d_model/2,)\n",
    "\n",
    "        pe = pe[:,0::2].assign(tf.math.sin(position*div_term))\n",
    "        pe = pe[:,1::2].assign(tf.math.cos(position*div_term))\n",
    "\n",
    "        self.pe = pe[tf.newaxis,:] # pe = (1,max_len,d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # inputs = (batch_size, seqs, channels)\n",
    "        return self.pe[:,:tf.shape(x)[1]] # (1,seqs,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = tf.keras.layers.Conv1D(filters=d_model, kernel_size=3, padding='valid', use_bias=False, kernel_initializer=tf.keras.initializers.HeUniform())\n",
    "\n",
    "    def call(self, x):\n",
    "        # x = (batch_size, seqs, channels) \n",
    "        x = tf.concat([x[:,-1][:,tf.newaxis], x, x[:,0][:,tf.newaxis]], axis=1) # x = (batch_size, seqs, d_model)\n",
    "        x = self.tokenConv(x) # x = (batch_size, seqs, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEmbedding(layers.Layer):\n",
    "    def __init__(self,d_model, dropout=0.0):\n",
    "        super(DataEmbedding,self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate = dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        # inputs = (batch_size, seqs, channels)\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) # x = (batch_size, seqs, d_model)\n",
    "        return self.dropout(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularCasualMask():\n",
    "    def __init__(self, B, L):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        self._mask = tf.experimental.numpy.triu(tf.ones(mask_shape), k=1)\n",
    "    \n",
    "    @property\n",
    "    def mask(self):\n",
    "        return tf.cast(self._mask, tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAttention(layers.Layer):\n",
    "    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = tf.keras.layers.Dropout(rate = attention_dropout)\n",
    "        window_size = win_size\n",
    "        distances = tf.Variable(tf.zeros([window_size,window_size]), trainable=False)\n",
    "        for i in range(window_size):\n",
    "            for j in range(window_size):\n",
    "                distances = distances[i,j].assign(abs(i-j)) # distances = (seqs, seqs)\n",
    "        self.distances = tf.constant(distances)\n",
    "    \n",
    "    def call(self, queries, keys, values, sigma, attn_mask):\n",
    "        # queries, keys, values = (batch_size, seqs, n_heads, d_keys), sigma = (batch_size, seqs, n_heads)\n",
    "        B, L, H, E = queries.shape # L=seqs, H=n_heads, E=d_keys\n",
    "        _, S, _, D = values.shape # S=seqs, D=d_keys\n",
    "        scale = self.scale or 1./ math.sqrt(E)\n",
    "\n",
    "        scores = tf.einsum(\"blhe,bshe->bhls\", queries, keys) # scores = (batch_size, n_heads, seqs, seqs)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCasualMask(B, L)\n",
    "            scores = tf.where(attn_mask.mask, -np.inf, scores)\n",
    "        attn = scale * scores # attn = (batch_size, n_heads, seqs, seqs)\n",
    "\n",
    "        sigma = tf.transpose(sigma, perm=[0,2,1]) # sigma = (batch_size, n_heads, seqs)\n",
    "        window_size = tf.shape(attn)[-1] # window_size = seqs\n",
    "        sigma = tf.math.sigmoid(sigma*5) + 1e-5\n",
    "        sigma = tf.math.pow(3,sigma) - 1\n",
    "        sigma = tf.tile(sigma[...,tf.newaxis],[1,1,1,window_size]) # sigma = (batch_size, n_heads, seqs, seqs)\n",
    "        prior = tf.tile(self.distances[tf.newaxis,...][tf.newaxis,...], [sigma.shape[0],sigma.shape[1],1,1]) # prior = (batch_size, n_heads, seqs, seqs)\n",
    "        prior = 1.0 / (math.sqrt(2*math.pi)*sigma)*tf.exp(-prior**2/2/(sigma**2)) # (batch_size, n_heads, seqs, seqs)\n",
    "\n",
    "        series = self.dropout(tf.nn.softmax(attn, axis=-1)) # series = (n_heads, seqs,seqs)\n",
    "        V = tf.einsum(\"bhls,bshd->blhd\", series, values) # V = (batch_size, seqs, n_heads, d_keys)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V, series, prior, sigma)\n",
    "        else:\n",
    "            return (V, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = tf.keras.layers.Dense(d_keys*n_heads)\n",
    "        self.key_projection = tf.keras.layers.Dense(d_keys*n_heads)\n",
    "        self.value_projection = tf.keras.layers.Dense(d_values*n_heads)\n",
    "        self.sigma_projection = tf.keras.layers.Dense(n_heads)\n",
    "        self.out_projection = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def call(self, queries, keys, values, attn_mask):\n",
    "        # queries = keys = values = (batch_size, seqs, d_model)\n",
    "        B, L, _ = queries.shape # L = seqs\n",
    "        _, S, _ = keys.shape # S = seqs\n",
    "        H = self.n_heads\n",
    "        x = queries # x = (batch_size, seqs, d_model)\n",
    "        queries = tf.reshape(self.query_projection(queries),[B,L,H,-1]) # (batch_size, seqs, d_model) -> (batch_size, seqs, n_heads, d_keys)\n",
    "        keys = tf.reshape(self.key_projection(keys), [B,S,H,-1]) # (batch_size, seqs, d_model) -> (batch_size, seqs, n_heads, d_keys)\n",
    "        values = tf.reshape(self.value_projection(values), [B, S,H,-1]) # (batch_size, seqs, d_model) -> (batch_size, seqs, n_heads, d_keys)\n",
    "        sigma = tf.reshape(self.sigma_projection(x), [B,L,H]) # (batch_size, seqs, n_heads) -> (batch_size, seqs, n_heads)\n",
    "\n",
    "        out, series, prior, sigma = self.inner_attention(queries, keys, values, sigma, attn_mask) # out=(seqs,n_heads,d_keys) series,prior,sigma=(n_heads,seqs,seqs)\n",
    "        out = tf.reshape(out,[B, L,-1]) # out = (batch_size, seqs, d_model)\n",
    "\n",
    "        return self.out_projection(out), series, prior, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation='relu'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = tf.keras.layers.Conv1D(d_ff, kernel_size=1)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(d_model, kernel_size=1)\n",
    "        self.norm1 = keras.layers.LayerNormalization()\n",
    "        self.norm2 = keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.activation = tf.nn.relu if activation == 'relu' else tf.nn.gelu\n",
    "\n",
    "    def call(self, x, attn_mask=None):\n",
    "        # x = (batch_size, seqs, d_model)\n",
    "        new_x, attn, mask, sigma = self.attention(x,x,x,attn_mask=attn_mask) # new_x=(batch_size,seqs,d_model) attn=(batch_size,n_heads,seqs,seqs) mask=(batch_size,n_heads,seqs,seqs) sigma=(batch_size,n_heads,seqs,seqs)\n",
    "        x = x + self.dropout(new_x) # x = (batch_size, seqs, d_model)\n",
    "        y = x = self.norm1(x) # y = (batch_size, seqs, d_model)\n",
    "        y = self.dropout(self.activation(self.conv1(y))) # y = (batch_size, seqs, d_model)\n",
    "        y = self.dropout(self.conv2(y)) # y = (batch_size, seqs, d_model)\n",
    "\n",
    "        return self.norm2(x+y), attn, mask, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, attn_layers, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.norm = norm_layer\n",
    "        self.attn_layers = attn_layers\n",
    "\n",
    "    def call(self, x, attn_mask=None):\n",
    "        # x = (batch_size, seqs, d_model)\n",
    "        series_list = list()\n",
    "        prior_list = list()\n",
    "        sigma_list = list()\n",
    "        for attn_layer in self.attn_layers.layers:\n",
    "            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)\n",
    "            series_list.append(series)\n",
    "            prior_list.append(prior)\n",
    "            sigma_list.append(sigma)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x, series_list, prior_list, sigma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTransformer(tf.keras.Model):\n",
    "    def __init__(self, win_size, c_out, d_model=512, n_heads=8, e_layers=3, d_ff=512, dropout=0.0, activation='gelu', output_attention=True):\n",
    "        super(AnomalyTransformer, self).__init__()\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        self.embedding = DataEmbedding(d_model, dropout)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        AnomalyAttention(win_size, False, attention_dropout=dropout,output_attention=output_attention), \n",
    "                        d_model, n_heads),\n",
    "                    d_model, \n",
    "                    d_ff, \n",
    "                    dropout=dropout, \n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ], \n",
    "            norm_layer=tf.keras.layers.LayerNormalization()\n",
    "        )\n",
    "\n",
    "        self.projection = tf.keras.layers.Dense(c_out)\n",
    "    \n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x, series, prior, sigmas = self.encoder(x)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return x, series, prior, sigmas\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(p,q):\n",
    "    res = p*(tf.math.log(p+0.0001) - tf.math.log(q+0.0001))\n",
    "    return tf.reduce_mean(tf.reduce_sum(res, axis=-1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = pr.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, label_set = dataloader.SMD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataloader.min_max(train_set, test_set)\n",
    "train_set_overlap = dataloader.window_overlap(train_set,100,0)\n",
    "test_set_overlap = dataloader.window_overlap(test_set,100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4968, 100, 51)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_overlap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnomalyTransformer(win_size=train_set_overlap.shape[1],c_out=train_set_overlap.shape[-1], e_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: -46.213966\n",
      "rec: 0.075172015\n",
      "prior: 15.429713\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    copy_set = np.copy(train_set_overlap)\n",
    "    for k in tqdm(range(copy_set.shape[0]//batch_size)):\n",
    "        loss1_list = list()\n",
    "        rec_loss_list = list()\n",
    "        prior_loss_list = list()\n",
    "        batch_mask = np.random.choice(copy_set.shape[0], batch_size, replace=False)\n",
    "        batch = copy_set[batch_mask]\n",
    "        series_loss = 0.0\n",
    "        prior_loss = 0.0\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            input = tf.constant(batch, dtype='float32')\n",
    "            output, series, prior, _ = model(input)\n",
    "            # output = model(input)\n",
    "            for u in range(len(prior)):\n",
    "                series_loss += tf.reduce_mean(kl_loss(series[u],\n",
    "                tf.stop_gradient((prior[u] / tf.tile(tf.reduce_sum(prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100]))))) \\\n",
    "                + tf.reduce_mean(kl_loss(tf.stop_gradient((prior[u] / tf.tile(tf.reduce_sum(prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100]))), series[u]))\n",
    "                prior_loss += tf.reduce_mean(kl_loss(tf.stop_gradient(series[u]), \\\n",
    "                (prior[u] / tf.tile(tf.reduce_sum(prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100])))) \\\n",
    "                + tf.reduce_mean(kl_loss((prior[u] / tf.tile(tf.reduce_sum(prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100])), tf.stop_gradient(series[u])))\n",
    "            series_loss = series_loss/len(series)\n",
    "            prior_loss = prior_loss/len(prior)\n",
    "            rec_loss = mse(output, input)\n",
    "            loss1 = rec_loss - 3*series_loss\n",
    "            loss2 = rec_loss + 3*prior_loss\n",
    "            loss1_list.append(loss1.numpy())\n",
    "            rec_loss_list.append(rec_loss.numpy())\n",
    "            prior_loss_list.append(prior_loss.numpy())\n",
    "        gradients1 = tape.gradient(loss1, model.trainable_variables)\n",
    "        gradients2 = tape.gradient(loss2, model.trainable_variables)\n",
    "        optimizer.apply_gradients((grad,var) for (grad,var) in zip(gradients1, model.trainable_variables) if grad is not None)\n",
    "        optimizer.apply_gradients((grad,var) for (grad,var) in zip(gradients2, model.trainable_variables) if grad is not None)\n",
    "        copy_set = np.delete(copy_set,batch_mask,axis=0)\n",
    "    display.clear_output()\n",
    "    print(\"loss1:\", np.average(loss1_list))\n",
    "    print(\"rec:\", np.average(rec_loss_list))\n",
    "    print(\"prior:\", np.average(prior_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_test(x,y):\n",
    "    return (x-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attens_energy = list()\n",
    "test_batch_size = 100\n",
    "num = test_set_overlap.shape[0] // test_batch_size\n",
    "for i in range(num):\n",
    "    test_input = test_set_overlap[i*100:(i+1)*100]\n",
    "    test_output, test_series, test_prior, _ = model(test_input)\n",
    "    test_loss = tf.reduce_mean(mse_test(test_input, test_output), axis=-1)\n",
    "    for u in range(len(test_prior)):\n",
    "        if u == 0:\n",
    "            test_series_loss = kl_loss(test_series[u], (test_prior[u] / tf.tile(tf.reduce_sum(test_prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100])))*50\n",
    "            test_prior_loss = kl_loss(test_prior[u] / tf.tile(tf.reduce_sum(test_prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100]), test_series[u])*50\n",
    "        else:\n",
    "            test_series_loss += kl_loss(test_series[u], (test_prior[u] / tf.tile(tf.reduce_sum(test_prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100])))*50\n",
    "            test_prior_loss += kl_loss(test_prior[u] / tf.tile(tf.reduce_sum(test_prior[u], axis=-1)[...,tf.newaxis], [1,1,1,100]), test_series[u])*50\n",
    "    metric = tf.nn.softmax((-test_prior_loss-test_series_loss), axis=-1)\n",
    "    cri = metric*test_loss\n",
    "    attens_energy.append(cri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_score = np.sort(attens_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "best_thr = 0\n",
    "gt = label_set\n",
    "for i in range(0,100):\n",
    "    thr = i/500\n",
    "    pred = (attens_energy > thr).astype(int)\n",
    "    anomaly_state = False\n",
    "    for i in range(len(pred)):\n",
    "        if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "            anomaly_state = True\n",
    "            for j in range(i, 0, -1):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "            for j in range(i, len(gt)):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "        elif gt[i] == 0:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            pred[i] = 1\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(label_set[:attens_energy.shape[0]],pred, average='binary')\n",
    "    if f_score > best_f:\n",
    "        best_f = f_score\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "        best_thr = thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9342781442822341\n",
      "recall: 0.9926319170240502\n",
      "f_score: 0.9625714495090135\n",
      "thr: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"precision:\", best_precision)\n",
    "print(\"recall:\", best_recall)\n",
    "print(\"f_score:\", best_f)\n",
    "print(\"thr:\", best_thr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6035c0200a6a71b9b0984fd1fa045b6d3ba8481bbe0e0d4a2f3dbba709d74662"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
