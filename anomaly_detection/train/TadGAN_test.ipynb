{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from functools import partial\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import processing as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(Layer):\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64,1,1))\n",
    "        return (alpha)*inputs[0] + (1-alpha)*inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_layer(input_shape, encoder_reshape_shape):\n",
    "\n",
    "    input_layer = layers.Input(shape=(input_shape))\n",
    "\n",
    "    x = layers.Bidirectional(LSTM(units=100, return_sequences=True))(input_layer)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(encoder_reshape_shape[0]*encoder_reshape_shape[1])(x)\n",
    "    x = layers.Reshape(target_shape=encoder_reshape_shape)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='encoder')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_layer(input_shape, generator_reshape_shape):\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(generator_reshape_shape[0]*generator_reshape_shape[1])(x)\n",
    "    x = layers.Reshape(target_shape=generator_reshape_shape)(x)\n",
    "    x = layers.Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(x)\n",
    "    x = layers.UpSampling1D(size=2)(x)\n",
    "    x = layers.Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(input_shape[-1]))(x)\n",
    "    x = layers.Activation(activation='tanh')(x)\n",
    "    model = keras.models.Model(input_layer, x, name='generator')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_x_layer(input_shape):\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(input_layer)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_x')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_z_layer(input_shape):\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)\n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)\n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_z')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = 100\n",
    "channels = 38\n",
    "latent_dim = 20\n",
    "shape = (seqs,channels)\n",
    "target_shape = (seqs,channels)\n",
    "\n",
    "encoder_input_shape = (seqs, channels)\n",
    "generator_input_shape = (seqs//5, channels)\n",
    "critic_x_input_shape = (seqs, channels)\n",
    "critic_z_input_shape = (seqs//5, channels)\n",
    "encoder_reshape_shape = (seqs//5, channels)\n",
    "generator_reshape_shape = (seqs//2, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder_layer(input_shape=encoder_input_shape, encoder_reshape_shape=encoder_reshape_shape)\n",
    "generator = build_generator_layer(input_shape=generator_input_shape, generator_reshape_shape=generator_reshape_shape)\n",
    "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
    "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.trainable = False\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=shape)\n",
    "y = Input(shape=target_shape)\n",
    "z = Input(shape=(latent_dim, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = generator(z)\n",
    "z_ = encoder(x)\n",
    "fake_x = critic_x(x_)\n",
    "valid_x = critic_x(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolated_x = RandomWeightedAverage()([y, x_])\n",
    "alpha1 = K.random_uniform((64,1,1))\n",
    "interpolated_x = y*alpha1 + x_*(1-alpha1)\n",
    "validity_interpolated_x = critic_x(interpolated_x)\n",
    "partial_gp_loss_x = partial(gradient_penalty_loss, averaged_samples=interpolated_x)\n",
    "partial_gp_loss_x.__name__ = 'gradient_penalty'\n",
    "critic_x_model = Model(inputs=[y, z], outputs=[valid_x, fake_x,\n",
    "                                                    validity_interpolated_x])\n",
    "critic_x_model.compile(loss=[wasserstein_loss, wasserstein_loss,\n",
    "                                    partial_gp_loss_x], optimizer=optimizer,\n",
    "                            loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_z = critic_z(z_)\n",
    "valid_z = critic_z(z)\n",
    "alpha2 = K.random_uniform((64,1,1))\n",
    "# interpolated_z = RandomWeightedAverage()([z, z_])\n",
    "interpolated_z = z*alpha2 + z_*(1-alpha2)\n",
    "validity_interpolated_z = critic_z(interpolated_z)\n",
    "partial_gp_loss_z = partial(gradient_penalty_loss, averaged_samples=interpolated_z)\n",
    "partial_gp_loss_z.__name__ = 'gradient_penalty'\n",
    "critic_z_model = Model(inputs=[x, z], outputs=[valid_z, fake_z,\n",
    "                                                    validity_interpolated_z])\n",
    "critic_z_model.compile(loss=[wasserstein_loss, wasserstein_loss,\n",
    "                                    partial_gp_loss_z], optimizer=optimizer,\n",
    "                            loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_x.trainable = False\n",
    "critic_z.trainable = False\n",
    "generator.trainable = True\n",
    "encoder.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen = Input(shape=(latent_dim, channels))\n",
    "x_gen_ = generator(z_gen)\n",
    "x_gen = Input(shape=shape)\n",
    "z_gen_ = encoder(x_gen)\n",
    "x_gen_rec = generator(z_gen_)\n",
    "fake_gen_x = critic_x(x_gen_)\n",
    "fake_gen_z = critic_z(z_gen_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])\n",
    "encoder_generator_model.compile(loss=[wasserstein_loss, wasserstein_loss,\n",
    "                                            'mse'], optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 35\n",
    "iterations_critic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = pr.data_loader()\n",
    "\n",
    "train_set, test_set, label_set = dataloader.SMD()\n",
    "\n",
    "train_set, test_set = dataloader.min_max(train_set, test_set)\n",
    "\n",
    "train_set_overlap = dataloader.window_overlap(train_set,100,0)\n",
    "test_set_overlap = dataloader.window_overlap(test_set,100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set_overlap\n",
    "target = train_set_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\minseok\\AppData\\Local\\Temp/ipykernel_25056/1252378524.py\", line 2, in gradient_penalty_loss  *\n        gradients = K.gradients(y_pred, averaged_samples)[0]\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 4263, in gradients  **\n        return tf.compat.v1.gradients(\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n        return array(a, dtype, copy=False, order=order)\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 255, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(64, 100, 38), dtype=tf.float32, name=None), name='tf.__operators__.add/AddV2:0', description=\"created by layer 'tf.__operators__.add'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25056/1380091016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             epoch_cx_loss.append(\n\u001b[1;32m---> 27\u001b[1;33m                 critic_x_model.train_on_batch([y, z], [valid, fake, delta]))\n\u001b[0m\u001b[0;32m     28\u001b[0m             epoch_cz_loss.append(\n\u001b[0;32m     29\u001b[0m                 critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
      "\u001b[1;32mc:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1898\u001b[0m                                                     class_weight)\n\u001b[0;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1900\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minseok\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minseok\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\minseok\\AppData\\Local\\Temp/ipykernel_25056/1252378524.py\", line 2, in gradient_penalty_loss  *\n        gradients = K.gradients(y_pred, averaged_samples)[0]\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 4263, in gradients  **\n        return tf.compat.v1.gradients(\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n        return array(a, dtype, copy=False, order=order)\n    File \"c:\\Users\\minseok\\anaconda3\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 255, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(64, 100, 38), dtype=tf.float32, name=None), name='tf.__operators__.add/AddV2:0', description=\"created by layer 'tf.__operators__.add'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "fake = np.ones((batch_size, 1))\n",
    "valid = -np.ones((batch_size, 1))\n",
    "delta = np.ones((batch_size, 1))\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = X[indices]\n",
    "    y_ = target[indices]\n",
    "\n",
    "    epoch_g_loss = []\n",
    "    epoch_cx_loss = []\n",
    "    epoch_cz_loss = []\n",
    "\n",
    "    minibatches_size = batch_size * iterations_critic\n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "\n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        y_minibatch = y_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "\n",
    "        for j in range(iterations_critic):\n",
    "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            y = y_minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            z = np.random.normal(size=(batch_size, latent_dim, channels))\n",
    "            epoch_cx_loss.append(\n",
    "                critic_x_model.train_on_batch([y, z], [valid, fake, delta]))\n",
    "            epoch_cz_loss.append(\n",
    "                critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "\n",
    "        epoch_g_loss.append(\n",
    "            encoder_generator_model.train_on_batch([x, z], [valid, valid, y]))\n",
    "\n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(\n",
    "        epoch, epochs, cx_loss, cz_loss, g_loss))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6035c0200a6a71b9b0984fd1fa045b6d3ba8481bbe0e0d4a2f3dbba709d74662"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
